{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "228bcc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max410011_l/distribution-analysis/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Step 1: Load the quantized model and tokenizer\n",
    "# MODEL_ID = \"TinyLlama-1.1B-Chat-v1.0-Smooth-GPTQ-SYM-W8A8-Dynamic-Per-Token\"\n",
    "MODEL_ID = \"TinyLlama-1.1B-Chat-v1.0-Smooth-GPTQ-ASYM-W8A8-Dynamic-Per-Token\"\n",
    "# MODEL_ID = \"TinyLlama-1.1B-Chat-v1.0-W8A8-Dynamic-Per-Token\"\n",
    "# MODEL_ID = \"TinyLlama-1.1B-Chat-v1.0-W8A8-Static-Per-Token\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Step 1: Load the quantized model and tokenizer (vLLM)\n",
    "# import os\n",
    "# from vllm import LLM\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# model = LLM(\"./TinyLlama-1.1B-Chat-v1.0-W8A8-Dynamic-Per-Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17812127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4/4 [00:00<00:00, 186.04 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 193.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Step 2: Prepare Calibration Data\n",
    "NUM_CALIBRATION_SAMPLES=4\n",
    "MAX_SEQUENCE_LENGTH=2048\n",
    "\n",
    "# Load dataset.\n",
    "ds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=f\"train_sft[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "ds = ds.shuffle(seed=42)\n",
    "\n",
    "# Preprocess the data into the format the model is trained with.\n",
    "def preprocess(example):\n",
    "    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False,)}\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# Tokenize the data (be careful with bos tokens - we need add_special_tokens=False since the chat_template already added it).\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample[\"text\"], padding=False, max_length=MAX_SEQUENCE_LENGTH, truncation=True, add_special_tokens=False)\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd265f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample: 0, length: torch.Size([1, 499])\n",
      "Processing sample: 1, length: torch.Size([1, 2048])\n",
      "Processing sample: 2, length: torch.Size([1, 1799])\n",
      "Processing sample: 3, length: torch.Size([1, 787])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Example forward pass to trigger the hook\n",
    "for i, sample in enumerate(ds):\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "        print(f\"Processing sample: {i}, length: {input_ids.shape}\")\n",
    "        _ = model(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab05c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "layer_distributions = {}\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, \"weight\") and hasattr(module, \"weight_scale\"):\n",
    "        weight = module.weight.data.detach().cpu()\n",
    "        weight_scale = module.weight_scale.detach().float().cpu()\n",
    "        weight_int8 = (weight / weight_scale).detach().int().cpu().numpy().flatten()\n",
    "        \n",
    "        inputs = np.concatenate([\n",
    "            inp.flatten().cpu().numpy()\n",
    "            for inp in module.inputs\n",
    "        ])\n",
    "        inputs_int8 = np.concatenate([\n",
    "            inp.flatten().cpu().numpy()\n",
    "            for inp in module.quantized_inputs\n",
    "        ])\n",
    "        \n",
    "        layer_distributions[name] = {\n",
    "            \"weight_scale\": weight_scale,\n",
    "            \"weight_int8\": weight_int8,\n",
    "            \"inputs\": inputs,\n",
    "            \"inputs_scales\": module.input_scales,\n",
    "            \"inputs_int8\": inputs_int8,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24127fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "os.makedirs(f\"output\", exist_ok=True)\n",
    "\n",
    "csv_rows = [\n",
    "    [\n",
    "        \"layer\",\n",
    "        \"weight_zero_pct\", \"weight_neg1_pct\", \"weight_pos1_pct\",\n",
    "        \"input_zero_pct\", \"input_neg1_pct\", \"input_pos1_pct\",\n",
    "        \"input_scale_min\", \"input_scale_max\", \"input_scale_mean\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for k, v in layer_distributions.items():\n",
    "    weight = v[\"weight_int8\"]\n",
    "    inputs = v[\"inputs_int8\"]\n",
    "\n",
    "    # Weight stats\n",
    "    weight_zero_pct = (weight == 0).sum().item() / weight.size\n",
    "    weight_neg1_pct = (weight == -1).sum().item() / weight.size\n",
    "    weight_pos1_pct = (weight == 1).sum().item() / weight.size\n",
    "\n",
    "    # Input stats\n",
    "    input_zero_pct = (inputs == 0).sum().item() / inputs.size\n",
    "    input_neg1_pct = (inputs == -1).sum().item() / inputs.size\n",
    "    input_pos1_pct = (inputs == 1).sum().item() / inputs.size\n",
    "\n",
    "    inp_scales = np.concatenate([s.numpy().flatten() for s in v[\"inputs_scales\"] if hasattr(s, \"numpy\")])\n",
    "    inp_scales_inv = 1.0 / inp_scales\n",
    "    input_scale_min = inp_scales_inv.min()\n",
    "    input_scale_max = inp_scales_inv.max()\n",
    "    input_scale_mean = inp_scales_inv.mean()\n",
    "\n",
    "    csv_rows.append([\n",
    "        k,\n",
    "        weight_zero_pct, weight_neg1_pct, weight_pos1_pct,\n",
    "        input_zero_pct, input_neg1_pct, input_pos1_pct,\n",
    "        input_scale_min, input_scale_max, input_scale_mean\n",
    "    ])\n",
    "\n",
    "with open(f\"output/{MODEL_ID}_layer_distribution.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(csv_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2eccee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(f\"figures/{MODEL_ID}\", exist_ok=True)\n",
    "\n",
    "for layer_name, v in layer_distributions.items():\n",
    "    weight = v[\"weight_int8\"]\n",
    "    inputs = v[\"inputs_int8\"]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(weight, bins=256, range=(-128,127), color='blue', alpha=0.7, density=True)\n",
    "    plt.title(f\"{layer_name} Weight Distribution\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y*100:.2f}%'))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(inputs, bins=256, range=(-128,127), color='green', alpha=0.7, density=True)\n",
    "    plt.title(f\"{layer_name} Input Distribution\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y*100:.2f}%'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/{MODEL_ID}/{layer_name}_distributions.png\", dpi=500)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distribution-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
