{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "228bcc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Step 1: Load the quantized model and tokenizer\n",
    "# MODEL_ID = \"TinyLlama-1.1B-Chat-v1.0-Smooth-GPTQ-W8A8-Dynamic-Per-Token\"\n",
    "# MODEL_ID = \"TinyLlama-1.1B-Chat-v1.0-Smooth-GPTQ-ASYM-W8A8-Dynamic-Per-Token\"\n",
    "MODEL_ID = \"TinyLlama-1.1B-Chat-v1.0-Smooth-GPTQ-FP8_DYNAMIC-Per-Token\"\n",
    "# MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=\"auto\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16)\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7defbf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using quantized model with CompressedLinear layers\n"
     ]
    }
   ],
   "source": [
    "# Setup input collection for model analysis\n",
    "import torch.nn as nn\n",
    "from compressed_tensors.linear.compressed_linear import CompressedLinear\n",
    "\n",
    "# Check for quantized layers and setup input collection accordingly\n",
    "has_quantized = any(isinstance(m, CompressedLinear) for _, m in model.named_modules())\n",
    "\n",
    "if has_quantized:\n",
    "    print(\"Using quantized model with CompressedLinear layers\")\n",
    "else:\n",
    "    # Add input collection hooks to Linear layers (excluding output layer)\n",
    "    def collect_inputs(module, inputs, _):\n",
    "        if not hasattr(module, 'inputs'):\n",
    "            module.inputs = []\n",
    "        module.inputs.append(inputs[0].detach().clone())\n",
    "    \n",
    "    linear_layers = [(n, m) for n, m in model.named_modules() \n",
    "                    if isinstance(m, nn.Linear) and 'lm_head' not in n]\n",
    "    \n",
    "    hooks = [module.register_forward_hook(collect_inputs) for _, module in linear_layers]\n",
    "    print(f\"Added input collection to {len(hooks)} Linear layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17812127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Step 2: Prepare Calibration Data\n",
    "NUM_CALIBRATION_SAMPLES=4\n",
    "MAX_SEQUENCE_LENGTH=2048\n",
    "\n",
    "# Load dataset.\n",
    "ds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=f\"train_sft[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "ds = ds.shuffle(seed=42)\n",
    "\n",
    "# Preprocess the data into the format the model is trained with.\n",
    "def preprocess(example):\n",
    "    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False,)}\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# Tokenize the data (be careful with bos tokens - we need add_special_tokens=False since the chat_template already added it).\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample[\"text\"], padding=False, max_length=MAX_SEQUENCE_LENGTH, truncation=True, add_special_tokens=False)\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd265f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample: 0, length: torch.Size([1, 499])\n",
      "Processing sample: 1, length: torch.Size([1, 2048])\n",
      "Processing sample: 1, length: torch.Size([1, 2048])\n",
      "Processing sample: 2, length: torch.Size([1, 1799])\n",
      "Processing sample: 2, length: torch.Size([1, 1799])\n",
      "Processing sample: 3, length: torch.Size([1, 787])\n",
      "Processing sample: 3, length: torch.Size([1, 787])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Example forward pass to trigger the hook\n",
    "for i, sample in enumerate(ds):\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "        print(f\"Processing sample: {i}, length: {input_ids.shape}\")\n",
    "        _ = model(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab05c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from compressed_tensors.linear.compressed_linear import CompressedLinear\n",
    "import torch\n",
    "\n",
    "def to_native_numpy(tensor):\n",
    "    \"\"\"Convert tensor to numpy array, preserving original dtype when possible\"\"\"\n",
    "    if tensor.dtype == torch.bfloat16:\n",
    "        return tensor.float().cpu().numpy()\n",
    "    elif tensor.dtype == torch.float16:\n",
    "        return tensor.cpu().numpy()\n",
    "    elif tensor.dtype == torch.float32:\n",
    "        return tensor.cpu().numpy()\n",
    "    else:\n",
    "        return tensor.float().cpu().numpy()\n",
    "\n",
    "# Determine data format from MODEL_ID (global variables)\n",
    "is_fp8 = \"FP8\" in MODEL_ID\n",
    "is_int8 = \"W8A8\" in MODEL_ID and not is_fp8\n",
    "\n",
    "layer_distributions = {}\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, CompressedLinear) or (hasattr(module, 'inputs') and hasattr(module, 'weight')):\n",
    "        if isinstance(module, CompressedLinear):\n",
    "            weight = module.weight.data.detach()\n",
    "            weight_scale = module.weight_scale.detach()\n",
    "            weight_int8 = (weight / weight_scale).detach().int().cpu().numpy().flatten()\n",
    "            \n",
    "            inputs = np.concatenate([\n",
    "                to_native_numpy(inp.flatten())\n",
    "                for inp in module.inputs\n",
    "            ])\n",
    "            inputs_int8 = np.concatenate([\n",
    "                inp.flatten().cpu().numpy()\n",
    "                for inp in module.quantized_inputs\n",
    "            ])\n",
    "            \n",
    "            layer_distributions[name] = {\n",
    "                \"weight_scale\": to_native_numpy(weight_scale),\n",
    "                \"weight_int8\": weight_int8,\n",
    "                \"inputs\": inputs,\n",
    "                \"inputs_scales\": [to_native_numpy(s) for s in module.input_scales],\n",
    "                \"inputs_int8\": inputs_int8,\n",
    "            }\n",
    "        \n",
    "        elif hasattr(module, 'inputs'):\n",
    "            inputs = np.concatenate([\n",
    "                to_native_numpy(inp.flatten())\n",
    "                for inp in module.inputs\n",
    "            ])\n",
    "            \n",
    "            weight = to_native_numpy(module.weight.data.flatten())\n",
    "            \n",
    "            layer_distributions[name] = {\n",
    "                \"weight\": weight,\n",
    "                \"inputs\": inputs,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24127fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "os.makedirs(f\"output\", exist_ok=True)\n",
    "\n",
    "csv_rows = [\n",
    "    [\n",
    "        \"layer\",\n",
    "        \"weight_zero_pct\", \"weight_neg1_pct\", \"weight_pos1_pct\",\n",
    "        \"input_zero_pct\", \"input_neg1_pct\", \"input_pos1_pct\",\n",
    "        \"input_scale_min\", \"input_scale_max\", \"input_scale_mean\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for k, v in layer_distributions.items():\n",
    "    # Handle weights - use weight_int8 if available, otherwise use weight\n",
    "    if \"weight_int8\" in v:\n",
    "        weight = v[\"weight_int8\"]\n",
    "        # Weight stats for quantized weights (exact integer values)\n",
    "        weight_zero_pct = (weight == 0).sum().item() / weight.size\n",
    "        weight_neg1_pct = (weight == -1).sum().item() / weight.size\n",
    "        weight_pos1_pct = (weight == 1).sum().item() / weight.size\n",
    "    elif \"weight\" in v:\n",
    "        weight = v[\"weight\"]\n",
    "        # For float weights, direct comparison (exact values are rare in neural networks)\n",
    "        weight_zero_pct = (weight == 0.0).sum().item() / weight.size\n",
    "        weight_neg1_pct = (weight == -1.0).sum().item() / weight.size\n",
    "        weight_pos1_pct = (weight == 1.0).sum().item() / weight.size\n",
    "    else:\n",
    "        weight_zero_pct = \"\"\n",
    "        weight_neg1_pct = \"\"\n",
    "        weight_pos1_pct = \"\"\n",
    "\n",
    "    # Handle inputs - use inputs_int8 if available, otherwise use inputs\n",
    "    if \"inputs_int8\" in v:\n",
    "        inputs = v[\"inputs_int8\"]\n",
    "        # Input stats for quantized inputs (exact integer values)\n",
    "        input_zero_pct = (inputs == 0).sum().item() / inputs.size\n",
    "        input_neg1_pct = (inputs == -1).sum().item() / inputs.size\n",
    "        input_pos1_pct = (inputs == 1).sum().item() / inputs.size\n",
    "    elif \"inputs\" in v:\n",
    "        inputs = v[\"inputs\"]\n",
    "        # For float inputs, direct comparison\n",
    "        input_zero_pct = (inputs == 0.0).sum().item() / inputs.size\n",
    "        input_neg1_pct = (inputs == -1.0).sum().item() / inputs.size\n",
    "        input_pos1_pct = (inputs == 1.0).sum().item() / inputs.size\n",
    "    else:\n",
    "        input_zero_pct = \"\"\n",
    "        input_neg1_pct = \"\"\n",
    "        input_pos1_pct = \"\"\n",
    "\n",
    "    # Handle input scales\n",
    "    if \"inputs_scales\" in v and v[\"inputs_scales\"]:\n",
    "        # Handle both list of arrays and single arrays\n",
    "        if isinstance(v[\"inputs_scales\"], list):\n",
    "            inp_scales = np.concatenate([s.flatten() if hasattr(s, 'flatten') else s \n",
    "                                       for s in v[\"inputs_scales\"]])\n",
    "        else:\n",
    "            inp_scales = v[\"inputs_scales\"].flatten()\n",
    "        \n",
    "        inp_scales_inv = 1.0 / (inp_scales + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "        input_scale_min = inp_scales_inv.min()\n",
    "        input_scale_max = inp_scales_inv.max()\n",
    "        input_scale_mean = inp_scales_inv.mean()\n",
    "    else:\n",
    "        input_scale_min = \"\"\n",
    "        input_scale_max = \"\"\n",
    "        input_scale_mean = \"\"\n",
    "\n",
    "    csv_rows.append([\n",
    "        k,\n",
    "        weight_zero_pct, weight_neg1_pct, weight_pos1_pct,\n",
    "        input_zero_pct, input_neg1_pct, input_pos1_pct,\n",
    "        input_scale_min, input_scale_max, input_scale_mean\n",
    "    ])\n",
    "\n",
    "with open(f\"output/{MODEL_ID.split('/')[-1]}_layer_distribution.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(csv_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2eccee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(f\"figures/{MODEL_ID.split('/')[-1]}\", exist_ok=True)\n",
    "\n",
    "for layer_name, v in layer_distributions.items():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Handle weights\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if \"weight_int8\" in v:\n",
    "        weight = v[\"weight_int8\"]\n",
    "        weights = np.full(len(weight), 100.0 / len(weight), dtype=np.float32)\n",
    "        \n",
    "        if is_fp8:\n",
    "            # FP8 E4M3 format\n",
    "            plt.hist(weight, bins=897, range=(-448, 448), color='blue', alpha=0.7, weights=weights)\n",
    "            plt.title(f\"{layer_name} Weight Distribution (FP8)\")\n",
    "        else:\n",
    "            # INT8 format\n",
    "            plt.hist(weight, bins=256, range=(-128, 127), color='blue', alpha=0.7, weights=weights)\n",
    "            plt.title(f\"{layer_name} Weight Distribution (INT8)\")\n",
    "        plt.xlabel(\"Quantized Value\")\n",
    "        \n",
    "    elif \"weight\" in v:\n",
    "        weight = v[\"weight\"]\n",
    "        weights = np.full(len(weight), 100.0 / len(weight), dtype=np.float32)\n",
    "        plt.hist(weight, bins=1000, color='blue', alpha=0.7, weights=weights)\n",
    "        plt.title(f\"{layer_name} Weight Distribution (Float)\")\n",
    "        plt.xlabel(\"Float Value\")\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No weight data', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title(f\"{layer_name} Weight Distribution (No Data)\")\n",
    "        plt.xlabel(\"Value\")\n",
    "    \n",
    "    plt.ylabel(\"Percentage (%)\")\n",
    "    \n",
    "    # Handle inputs\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if \"inputs_int8\" in v:\n",
    "        inputs = v[\"inputs_int8\"]\n",
    "        weights = np.full(len(inputs), 100.0 / len(inputs), dtype=np.float32)\n",
    "        \n",
    "        if is_fp8:\n",
    "            # FP8 E4M3 format\n",
    "            plt.hist(inputs, bins=897, range=(-448, 448), color='green', alpha=0.7, weights=weights)\n",
    "            plt.title(f\"{layer_name} Input Distribution (FP8)\")\n",
    "        else:\n",
    "            # INT8 format\n",
    "            plt.hist(inputs, bins=256, range=(-128, 127), color='green', alpha=0.7, weights=weights)\n",
    "            plt.title(f\"{layer_name} Input Distribution (INT8)\")\n",
    "        plt.xlabel(\"Quantized Value\")\n",
    "        \n",
    "    elif \"inputs\" in v:\n",
    "        inputs = v[\"inputs\"]\n",
    "        weights = np.full(len(inputs), 100.0 / len(inputs), dtype=np.float32)\n",
    "        plt.hist(inputs, bins=1000, color='green', alpha=0.7, weights=weights)\n",
    "        plt.title(f\"{layer_name} Input Distribution (Float)\")\n",
    "        plt.xlabel(\"Float Value\")\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No input data', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title(f\"{layer_name} Input Distribution (No Data)\")\n",
    "        plt.xlabel(\"Value\")\n",
    "    \n",
    "    plt.ylabel(\"Percentage (%)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/{MODEL_ID.split('/')[-1]}/{layer_name.replace('/', '_')}_distributions.png\", dpi=300)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
